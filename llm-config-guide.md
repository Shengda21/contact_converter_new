# AI联系人转换器 - 自定义LLM配置指南

## 🎯 功能概览

应用现在支持两种AI引擎：
1. **Claude (默认)** - 最佳识别效果，无需配置
2. **自定义LLM** - 支持本地部署和第三方大模型

## 🔧 配置步骤

### 1. 打开LLM配置面板
- 点击输入区域的 **"⚙️ LLM配置"** 按钮
- 配置面板会展开显示当前设置

### 2. 选择AI引擎
- **🔮 使用Claude (默认)**: 推荐选项，识别准确度最高
- **🏠 使用自定义LLM**: 使用本地或第三方大模型

### 3. 配置自定义LLM (如选择)
填写以下信息：
- **API地址**: LLM服务的基础URL
- **API密钥**: 认证密钥（本地模型通常不需要）
- **模型名称**: 要使用的具体模型

## 🖥️ 常用本地LLM配置

### LM Studio
```
API地址: http://localhost:1234/v1
API密钥: (留空)
模型名称: llama-3-8b-instruct
```

### Ollama
```
API地址: http://localhost:11434/v1
API密钥: (留空)
模型名称: llama3:8b
```

### vLLM
```
API地址: http://localhost:8000/v1
API密钥: (留空)
模型名称: Qwen/Qwen2-7B-Instruct
```

## 🌐 第三方API配置

### OpenAI
```
API地址: https://api.openai.com/v1
API密钥: sk-xxx...
模型名称: gpt-4o
```

### 其他兼容OpenAI格式的服务
- **DeepSeek API**
- **智谱AI API** 
- **通义千问API**
- 等等...

## ✅ 使用建议

### 推荐配置
1. **默认使用Claude** - 最佳识别效果，专门优化过联系人信息提取
2. **备选本地LLM** - 隐私保护，离线使用

### 模型选择建议
- **7B-13B模型**: 基础信息提取，如Llama-3-8B
- **70B+模型**: 复杂文本理解，如Qwen2-72B
- **专业模型**: 针对中文优化的模型效果更好

### 性能优化
- 本地GPU加速：推荐使用CUDA/ROCm
- 量化模型：4bit/8bit量化降低显存需求
- 批量处理：建议使用批量模式提高效率

## 🛠️ 故障排除

### 常见问题
1. **连接失败**: 检查API地址和端口是否正确
2. **认证错误**: 验证API密钥格式和权限
3. **模型不存在**: 确认模型名称与服务器一致
4. **响应格式错误**: 确保API兼容OpenAI格式

### 调试步骤
1. 测试API连接性
2. 验证模型可用性
3. 检查请求格式
4. 查看错误日志

## 📈 效果对比

| 引擎类型 | 识别准确度 | 响应速度 | 隐私保护 | 成本 |
|---------|-----------|----------|----------|------|
| Claude | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | 💰💰 |
| 本地LLM | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 💰 |
| 第三方API | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐ | 💰💰💰 |

## 🎯 应用场景

### 适合使用Claude的场景
- 需要最高识别准确度
- 处理复杂格式的联系人信息
- 重要商务联系人整理

### 适合使用自定义LLM的场景
- 隐私敏感的联系人信息
- 大批量数据处理
- 离线环境使用
- 成本控制需求

配置完成后，应用会根据你的选择使用相应的AI引擎进行联系人信息提取和vCard生成！
